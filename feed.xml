<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://vidageek.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://vidageek.com/" rel="alternate" type="text/html" /><updated>2023-12-01T11:01:23+00:00</updated><id>https://vidageek.com/feed.xml</id><title type="html">VidaGeek</title><subtitle>Un podcast sobre tecnología, programación, videojuegos y mucho más</subtitle><entry><title type="html">Podcast 3x05</title><link href="https://vidageek.com/3/2023/12/01/podcast-3x05.html" rel="alternate" type="text/html" title="Podcast 3x05" /><published>2023-12-01T00:00:00+00:00</published><updated>2023-12-01T00:00:00+00:00</updated><id>https://vidageek.com/3/2023/12/01/podcast-3x05</id><content type="html" xml:base="https://vidageek.com/3/2023/12/01/podcast-3x05.html"><![CDATA[<h1 id="openhaystack-airtags-para-todos">OpenHaystack, AirTags para todos</h1>
<p>Hoy os hablamos sobre OpenHaystack, un proyecto de código abierto realizado por la universidad <a href="https://www.tu-darmstadt.de">TU Darmstadt</a> a partir de ingenieria inversa del protocolo de los AirTag. Nos permite poder crear nuestros propios AirTags y que funcionen dentro de la red FindMy de Apple. Además, jugando con el proyecto, he realizado un nuevo firmware de muy bajo consumo que funciona con los chips nrf52 de Nordic.</p>

<p>Aquí os dejo los enlaces del proyecto y del firmware de bajo consumo:</p>
<ul>
  <li>Proyecto OpenHaystack: <a href="https://github.com/seemoo-lab/openhaystack">https://github.com/seemoo-lab/openhaystack</a></li>
  <li>Firmware modificado de muy bajo consumo: <a href="https://github.com/acalatrava/openhaystack-firmware">https://github.com/acalatrava/openhaystack-firmware</a></li>
  <li>Firmware más avanzado que permite incluso actualización DFU: <a href="https://github.com/spaceinvadertech/openhaystack_moko">https://github.com/spaceinvadertech/openhaystack_moko</a></li>
</ul>

<p>Un saludo!</p>]]></content><author><name></name></author><category term="3" /><summary type="html"><![CDATA[OpenHaystack, AirTags para todos Hoy os hablamos sobre OpenHaystack, un proyecto de código abierto realizado por la universidad TU Darmstadt a partir de ingenieria inversa del protocolo de los AirTag. Nos permite poder crear nuestros propios AirTags y que funcionen dentro de la red FindMy de Apple. Además, jugando con el proyecto, he realizado un nuevo firmware de muy bajo consumo que funciona con los chips nrf52 de Nordic.]]></summary></entry><entry><title type="html">Podcast 3x04</title><link href="https://vidageek.com/3/2023/11/19/podcast-3x04.html" rel="alternate" type="text/html" title="Podcast 3x04" /><published>2023-11-19T00:00:00+00:00</published><updated>2023-11-19T00:00:00+00:00</updated><id>https://vidageek.com/3/2023/11/19/podcast-3x04</id><content type="html" xml:base="https://vidageek.com/3/2023/11/19/podcast-3x04.html"><![CDATA[<h1 id="sam-altman-es-despedido-y-readmitido">Sam Altman es despedido… ¿y readmitido?</h1>
<p>Este fin de semana OpenAI se ha levantado de resaca. El viernes a mediodía despedían a Sam Altman sin previo aviso… ¿pero parece ser que apenas 48 horas más tarde se han arrepentido?</p>

<h1 id="el-nuevo-dispositivo-ai-pin-quiere-ser-lo-que-le-siga-al-smartphone">El nuevo dispositivo AI Pin quiere ser lo que le siga al smartphone</h1>
<p>Humane ha presentado su nuevo AI Pin, un dispositivo móvil que no se parece a nada de lo que hayas visto antes… para bien y para mal. Está cargado de inteligencia artificial y pretende remplazar de alguna forma al smartphone.</p>

<h1 id="elon-musk-consigue-un-nuevo-hito-en-el-lanzamiento-de-starship">Elon Musk consigue un nuevo hito en el lanzamiento de Starship</h1>
<p>El último lanzamiento de Starship ha sido un éxito consiguiendo nuevos hitos. El avance de la misión va estupendamente.</p>

<p>Te lo contamos todo en este nuevo episodio, ¡Que lo disfrutes!</p>]]></content><author><name></name></author><category term="3" /><summary type="html"><![CDATA[Sam Altman es despedido… ¿y readmitido? Este fin de semana OpenAI se ha levantado de resaca. El viernes a mediodía despedían a Sam Altman sin previo aviso… ¿pero parece ser que apenas 48 horas más tarde se han arrepentido?]]></summary></entry><entry><title type="html">Podcast 3x03</title><link href="https://vidageek.com/3/2023/11/05/podcast-3x03.html" rel="alternate" type="text/html" title="Podcast 3x03" /><published>2023-11-05T00:00:00+00:00</published><updated>2023-11-05T00:00:00+00:00</updated><id>https://vidageek.com/3/2023/11/05/podcast-3x03</id><content type="html" xml:base="https://vidageek.com/3/2023/11/05/podcast-3x03.html"><![CDATA[<h1 id="copias-de-seguridad">Copias de seguridad</h1>
<p>En el episodio de hoy hablamos sobre copias de seguridad y sobre cómo confiar sólo en la nube, ya sea iCloud o cualquier otra, no es una buena idea. Descubriremos por qué la búsqueda de un sistema de backup le ha llevado a Antonio a suscribirse a una plataforma de videojuegos y explicamos cómo utilizar el servicio <a href="https://backblaze.com">Backblaze</a> para realizar copias de seguridad de nuestro equipo.</p>

<p>También comentamos cómo Antonio ha utilizado una herramienta de código abierto llamada <a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader">iCloud Photo Downloader</a> para descargar toda su librería de fotos de iCloud a un disco duro local.</p>

<hr />
<p>¡Que lo disfrutes!</p>]]></content><author><name></name></author><category term="3" /><summary type="html"><![CDATA[Copias de seguridad En el episodio de hoy hablamos sobre copias de seguridad y sobre cómo confiar sólo en la nube, ya sea iCloud o cualquier otra, no es una buena idea. Descubriremos por qué la búsqueda de un sistema de backup le ha llevado a Antonio a suscribirse a una plataforma de videojuegos y explicamos cómo utilizar el servicio Backblaze para realizar copias de seguridad de nuestro equipo.]]></summary></entry><entry><title type="html">Podcast 3x02</title><link href="https://vidageek.com/3/2023/10/03/podcast-3x02.html" rel="alternate" type="text/html" title="Podcast 3x02" /><published>2023-10-03T00:00:00+00:00</published><updated>2023-10-03T00:00:00+00:00</updated><id>https://vidageek.com/3/2023/10/03/podcast-3x02</id><content type="html" xml:base="https://vidageek.com/3/2023/10/03/podcast-3x02.html"><![CDATA[<h1 id="charlando-sobre-iphone-15-macbook-air-financiación-openai-bing-meta-quest-y-más">Charlando sobre iPhone 15, Macbook Air, financiación, OpenAI, Bing, Meta Quest y más</h1>
<p>En el episodio de hoy hablamos de forma distendida sobre varias cosas como el iPhone 15 o el Macbook Air de los cuales Paco es nuevo poseedor (aún en parte). Además hablamos sobre <a href="https://github.com/jzhang38/TinyLlama">TinyLlama</a>, un nuevo LLM “pequeño” de tan sólo 1.1 billones de parámetros.</p>

<p>También os contamos lo nuevo de <a href="https://openai.com/blog/chatgpt-can-now-see-hear-and-speak">OpenAI que ahora puede oír, hablar y ver</a> y de cómo podemos utilizarlo gratis gracias a <a href="https://www.bing.com/?cc=es">Bing</a>.</p>

<p>Meta ha presentado las nuevas <a href="https://www.meta.com/es/quest/quest-3/">Quest 3</a> y han hecho un vídeo en el nos muestran cómo las podemos utilizar para entrar <a href="https://www.youtube.com/watch?v=MVYrJJNdrEg">en una videoconferencia con un avatar fotorrealista</a>. Muy interesante, la verdad.</p>

<hr />
<p>Hasta la próxima semana!</p>]]></content><author><name></name></author><category term="3" /><summary type="html"><![CDATA[Charlando sobre iPhone 15, Macbook Air, financiación, OpenAI, Bing, Meta Quest y más En el episodio de hoy hablamos de forma distendida sobre varias cosas como el iPhone 15 o el Macbook Air de los cuales Paco es nuevo poseedor (aún en parte). Además hablamos sobre TinyLlama, un nuevo LLM “pequeño” de tan sólo 1.1 billones de parámetros.]]></summary></entry><entry><title type="html">Podcast 3x01</title><link href="https://vidageek.com/3/2023/09/25/podcast-3x01.html" rel="alternate" type="text/html" title="Podcast 3x01" /><published>2023-09-25T00:00:00+00:00</published><updated>2023-09-25T00:00:00+00:00</updated><id>https://vidageek.com/3/2023/09/25/podcast-3x01</id><content type="html" xml:base="https://vidageek.com/3/2023/09/25/podcast-3x01.html"><![CDATA[<h1 id="open-interpreter-hablando-con-tu-ordenador">Open Interpreter: Hablando con tu ordenador</h1>

<p>La inteligencia artificial sigue imparable. Hoy vamos a hablar de un nuevo proyecto de código abierto llamado <a href="https://github.com/KillianLucas/open-interpreter">Open Interpreter</a> el cual nos permite “hablar” con nuestro ordenador.</p>

<p><a href="https://github.com/KillianLucas/open-interpreter">Open Interpreter</a> es la respuesta de código abierto al plugin “Code Interpreter” que sacó en la versión plus de ChatGPT. Este plugin permite la creación y ejecución de código en base a una petición, o prompt. Entre otras cosas te permite resolución de problemas matemáticos, análisis y visualización de datos, convertir ficheros a diferentes formatos, y cosas así. No obstante tiene ciertas limitaciones como que opera sin acceso a internet, tiene un set de paquetes previamente instalados limitado, tiene un límite de 2 minutos de ejecución y el estado es eliminado después de la ejecución.</p>

<p>Open Interpreter viene a mejorar todo esto, dando acceso completo a tu PC o Mac de forma que pueda ejecutar código diréctamente en tu equipo y con acceso a internet, eliminando todos esos límites impuestos por ChatGPT. Además, al ser un proyecto open source, puede operar totalmente en local utilizando un LLM que también se ejecute en local. Aunque si queremos que tenga mucha más potencia y conocimiento lo ideal es que utilicemos GPT4. No obstante en este caso estaremos enviando todo lo que hacemos a OpenAI y esto puede suponer un problema de privacidad.</p>

<p>Aquí os vamos a explicar cómo ponerlo en marcha totalmente en local, sin tener que utilizar GPT4. Para ello utilizaremos CodeLlama, un LLM desarrollado por Meta, especializado en programación, que se ejecuta en local y con una licencia bastante abierta.</p>

<h2 id="puesta-en-marcha">Puesta en marcha</h2>

<p>Lo primero que tenemos que realizar es crearnos un entorno de ejecución con Conda. Si bien este paso no es esencial, es muy recomendable. Conda es un gestor de entornos de ejecución de forma que nos crea entornos independientes para cada proyecto. Esto está muy bien ya que muchas veces los proyectos necesitan de versiones específicas de ciertos paquetes o librerías. Para ello Conda crea un entorno de ejecución para cada proyecto aislado de los demas. Si bien aquí no vamos a tratar la instalación de Conda, es algo muy sencillo de realizar haciéndo una búsqueda en internet.</p>

<p>Una vez tengamos conda instalado crearemos un entorno para el proyecto:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ conda create python=3.11 open-interpreter
 $ conda activate open-interpreter
</code></pre></div></div>

<p>El siguiente paso es simplemente instalar open-interpreter con pip:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ pip install open-interpreter
</code></pre></div></div>

<p>Y ya está! Simplemente con esto ya tenemos Open Interpreter instalado. Ahora simplemente tenemos que ejecutar</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> $ interpreter
</code></pre></div></div>
<p>y seguir los pasos que nos va pidiendo.</p>

<h2 id="gpt4-o-codellama">GPT4 o CodeLlama</h2>

<p>Lo primero que nos pedirá es la clave de API de OpenAI para utilizar GPT4 como motor LLM. Esto es lo más rápido y efectivo puesto que GPT4 es el LLM más potente y capaz hasta la fecha. No obstante tiene ciertas desventajas:</p>
<ul>
  <li>No es gratis</li>
  <li>Todos tus comandos y respuestas serán enviados a OpenAI para procesarlos
Como alternativa podemos usar CodeLlama. Un LLM desarrollado por Meta que podemos ejecutar en local. CodeLlama viene en 3 sabores: 7B, 11B y 34B. Cada número se refiere a los billones de parámetros que tiene el modelo. Como suele ser en estas cosas, cuanto más, mejor. Pero cuanto más, más RAM y potencia de cálculo necesitarás.</li>
</ul>

<p>He hecho pruebas con el más potente (34B aunque muy comprimido) y con GPT4 y claramente GPT4 es mucho más potente y capaz. Aunque en la petición que le hice (generar un fichero excel a partir de un listado de películas en un PDF) no fue capaz de conseguir el objetivo final.</p>

<h2 id="opencore-legacy-patcher">OpenCore Legacy Patcher</h2>

<p>Paco nos ha comentado cómo ha sido posible actualizar su Macbook Pro Retina de 2014 a la última versión de MacOS gracias a <a href="https://dortania.github.io/OpenCore-Legacy-Patcher/">OpenCore Legacy Patcher</a>, un proyecto open source que nos permite crear un instalador de MacOS que engaña al sistema operativo haciéndole creer que se está ejecutando en un hardware soportado.</p>

<p>Dependiendo del hardware que tengamos algos OS serán soportados y otros no. Sin embargo si tu equipo es de 2013 o supuerior suele poder soportar los últimos MacOS hasta la fecha. Quizá haya algunas funcionalidades que no estén disponibles pero generalmente funciona francamente bien.</p>

<hr />

<p>¡Hasta la próxima semana!</p>]]></content><author><name></name></author><category term="3" /><summary type="html"><![CDATA[Open Interpreter: Hablando con tu ordenador]]></summary></entry><entry><title type="html">Podcast 2x14</title><link href="https://vidageek.com/2/2023/07/06/podcast-2x14.html" rel="alternate" type="text/html" title="Podcast 2x14" /><published>2023-07-06T00:00:00+00:00</published><updated>2023-07-06T00:00:00+00:00</updated><id>https://vidageek.com/2/2023/07/06/podcast-2x14</id><content type="html" xml:base="https://vidageek.com/2/2023/07/06/podcast-2x14.html"><![CDATA[<h1 id="hubble-network-revolucionando-la-comunicación-de-dispositivos-iot-desde-el-espacio">Hubble Network: Revolucionando la comunicación de dispositivos IoT desde el espacio</h1>

<p>Hoy vamos a hablar de Hubble Network, una compañía que pretende revolucionar la forma en la que se comunican muchos de nuestros dispositivos, especialmente aquellos relacionados con el Internet de las Cosas (IoT). Estos dispositivos suelen enfrentar un problema importante: la comunicación con Internet, la cual requiere de sistemas costosos en términos de batería y tamaño.</p>

<h2 id="el-desafío-de-la-comunicación-en-dispositivos-iot">El desafío de la comunicación en dispositivos IoT</h2>

<p>Imaginemos dispositivos ubicados en un campo, encargados de monitorear animales o condiciones específicas. Para enviar esta información periódicamente a Internet, se requiere de un sistema de comunicación que, en la mayoría de los casos, resulta costoso en términos de batería y cobertura. Por ejemplo, si utilizáramos un modem GSM o LTE, similar a un teléfono móvil, nos enfrentaríamos a dos problemas principales: el consumo de batería y la cobertura en zonas rurales remotas, donde la señal puede ser escasa o inexistente.</p>

<p>Existen otros sistemas más eficientes energéticamente, como SIGFOX o LORAWAN, pero su despliegue depende de la presencia de repetidores de estos protocolos en las cercanías. En algunas zonas, puede que estos repetidores no estén disponibles.</p>

<h2 id="la-revolución-de-hubble-network">La revolución de Hubble Network</h2>

<p>Es aquí donde Hubble Network busca revolucionar estos sistemas de comunicación de una forma sencilla y eficiente. Esta empresa tiene como objetivo llevar los dispositivos BLE (Bluetooth Low Energy) al espacio. Si estás escuchando este podcast con unos AirPods u otros auriculares inalámbricos, es muy probable que estés utilizando la tecnología BLE, la cual permite una conexión fácil entre estos dispositivos y nuestros móviles.</p>

<p>Hubble Network ha desarrollado un satélite con miles de antenas modificadas para capturar las señales de los dispositivos BLE que, con un firmware específico, transmiten desde la Tierra. Esta capacidad es impresionante, ya que elimina la necesidad de utilizar chips o módems especiales como SIGFOX o LORAWAN. La mayoría de los dispositivos de IoT ya cuentan con un chip BLE, y si no lo tienen, su incorporación resulta increíblemente económica y eficiente.</p>

<p>Por ejemplo, en un proyecto en el que he estado trabajando, se utilizó un sistema similar con un chip BLE de Nordic Semi, capaz de funcionar durante 2 años con una simple pila de botón. Un ejemplo conocido de este sistema es el AirTag de Apple, cuya autonomía también es muy alta.</p>

<h2 id="la-constelación-de-satélites-de-hubble-network">La constelación de satélites de Hubble Network</h2>

<p>Hubble Network planea poner en órbita baja una constelación de satélites que capturarán la información enviada por dispositivos Bluetooth. La empresa ha conseguido una inversión de 20 millones de dólares, lo cual le permitirá lanzar su primer satélite en enero de 2024, gracias a SpaceX, y comenzar a operar su red.</p>

<p>Durante los primeros meses, con un único satélite en órbita, se espera que la captura de datos de los dispositivos ocurra cada 6 horas, el tiempo que tardará en dar una vuelta completa alrededor de nuestro planeta. A pesar de las limitaciones iniciales, esto es un logro impresionante para el inicio de esta ambiciosa empresa. Seguiremos de cerca los avances de Hubble Network y os mantendremos informados sobre su desarrollo.</p>

<p>Si tengo la oportunidad de obtener el kit de desarrollo de esta tecnología, lo utilizaré para explorar su funcionamiento. Tengo muchas ganas de presenciar cómo Hubble Network está transformando la forma en que se comunican los dispositivos IoT desde el espacio.</p>]]></content><author><name></name></author><category term="2" /><summary type="html"><![CDATA[Hubble Network: Revolucionando la comunicación de dispositivos IoT desde el espacio]]></summary></entry><entry><title type="html">Podcast 2x13</title><link href="https://vidageek.com/2/2023/06/11/podcast-2x13.html" rel="alternate" type="text/html" title="Podcast 2x13" /><published>2023-06-11T00:00:00+00:00</published><updated>2023-06-11T00:00:00+00:00</updated><id>https://vidageek.com/2/2023/06/11/podcast-2x13</id><content type="html" xml:base="https://vidageek.com/2/2023/06/11/podcast-2x13.html"><![CDATA[<h1 id="la-visión-de-apple">La visión de Apple</h1>
<p>Todos los rumores que hubo previos al lanzamiento de las esperadas gafas de Apple se han cumplido. El pasado lunes 5 de Junio Apple puso sobre la mesa su nueva visión del futuro con un modelo de gafas que parece traído directamente de una película de ciencia ficción.</p>

<p>Y es que las nuevas gafas de Apple definen el futuro de la computación, o como ellos lo han llamado, la computación espacial. Un futuro en el que la pantalla no nos limitará en la interacción sino que podremos ampliarla o reducirla a nuestro antojo. O poner 3 pantallas si queremos.</p>

<p>Finalmente todas las especificaciones filtradas se cumplieron y tenemos unas gafas que tinen una pantalla superior a 4K en cada ojo con una densidad de píxeles superior a los 4000ppp, un procesador M2 junto a un coprocesador R1 que se encarga de la gestión de todos los sensores, que no son pocos. 12 cámaras, 6 micrófonos, seguimiento ocular y seguimiento de manos.</p>

<p>Estas gafas nos permiten crear un entorno virtual junto a nuestro entorno físico e interactuar con el mismo. Para ello no utilizamos controles externos sino que simplemente utilizamos nuestra vista y nuestras manos. Miramos lo que queremos seleccionar y pellizcamos con los dedos.</p>

<p>También tiene una corona digital con la cual controlaremos el nivel de inmersión en el sistema pudiendo ocultar por completo nuestro entorno real.</p>

<p>Además tiene una pantalla exterior en 3D que permite ver una recreación simulada de nuestros ojos de forma que da la sensación de que las gafas son transparentes.</p>

<p>La integración con el ecosistema es espectacular de forma que podemos “sacar” la pantalla de un Macbook y representarla en el espacio.</p>

<p>Lo malo es el precio, a partir de 3500$ más impuestos y sólo se pondrá a la venta en USA a partir del año que viene.</p>

<p>Tenemos muchas ganas de probarlas y veremos si somos capaces con hacernos con unas.</p>

<p>¡Un saludo y hasta la próxima semana!</p>]]></content><author><name></name></author><category term="2" /><summary type="html"><![CDATA[La visión de Apple Todos los rumores que hubo previos al lanzamiento de las esperadas gafas de Apple se han cumplido. El pasado lunes 5 de Junio Apple puso sobre la mesa su nueva visión del futuro con un modelo de gafas que parece traído directamente de una película de ciencia ficción.]]></summary></entry><entry><title type="html">Podcast 2x12</title><link href="https://vidageek.com/2/2023/06/03/podcast-2x12.html" rel="alternate" type="text/html" title="Podcast 2x12" /><published>2023-06-03T00:00:00+00:00</published><updated>2023-06-03T00:00:00+00:00</updated><id>https://vidageek.com/2/2023/06/03/podcast-2x12</id><content type="html" xml:base="https://vidageek.com/2/2023/06/03/podcast-2x12.html"><![CDATA[<h1 id="según-esta-filtración-las-gafas-de-apple-tendían-unas-especificaciones-loquísimas">Según esta filtración, las gafas de Apple tendían unas especificaciones loquísimas</h1>
<p>Y otro rumor más… por si teníamos pocos, este último te dejará con la boca abierta (o más bien con los ojos abiertos) ya que se han filtrado las especificaciones de lo que serán las gafas de Apple y no tienen competencia… Serían estas:</p>

<ul>
  <li>Pantalla Micro-OLED de 1,41 pulgadas para cada ojo</li>
  <li>Resolución 4K en cada ojo</li>
  <li>Densidad de píxeles de 4000 ppi (puntos por pulgada)</li>
  <li>Más de 5000 nits de brillo</li>
  <li>ProMotion display hasta 120hz de refresco</li>
</ul>

<p>En comparación las PlayStation VR2 tienen unas pantallas de 2000x2400 pixeles (frente a 3840x2160 del 4K) y una densidad de píxeles de 850ppi…</p>

<p>Así se explica que hubiera otro rumor que situaba su precio en unos 3000$…</p>

<p>Fuente: <a href="https://www.macworld.com/article/1935837/apple-reality-headset-display-leak-resolution-brightness.html?utm_source=tldrnewsletter">https://www.macworld.com/article/1935837/apple-reality-headset-display-leak-resolution-brightness.html?utm_source=tldrnewsletter</a></p>

<h3 id="por-cierto-que-el-lunes-a-partir-de-las-1845-hora-española-daremos-cobertura-en-directo-al-evento-de-apple">Por cierto que el lunes a partir de las 18:45 hora española daremos cobertura en directo al evento de Apple.</h3>
<h2 id="suscríbete-a-nuestro-canal-de-youtube-para-verlo-en-directo-httpswwwyoutubecomliveskn2xcvt0lofeatureshareaa">Suscríbete a nuestro canal de YouTube para verlo en directo: <a href="https://www.youtube.com/live/sKn2Xcvt0Lo?feature=share">https://www.youtube.com/live/sKn2Xcvt0Lo?feature=share</a>aa</h2>

<hr />

<h1 id="imagebind-un-embedding-para-enlazarlos-a-todos">ImageBind un embedding para enlazarlos a todos</h1>
<p>Meta sigue dándonos muchas alegrías ya que ha presentado un nuevo modelo de IA que pretende enlazar múltiples y diversos fuentes de datos como:</p>
<ul>
  <li>Audio</li>
  <li>Texto</li>
  <li>Imagen</li>
  <li>Datos de profundidad</li>
  <li>Datos de aceleración</li>
  <li>Mapas de calor</li>
</ul>

<p>Este nuevo modelo multimodal crea unos embeddings que interrelacionan todos los datos aunque sean de diferente tipo. De este modo se le puede pasar un audio de un perro ladrando y encontrar imágenes de perros por ejemplo. Esto abre un nuevo campo en la IA ya que enlazándolo con los grandes modelos de lenguaje LLM nos permitrá una interacción multimodal tremenda.</p>

<p>Además, es de código abierto. Meta nos está sorprendiendo mucho en este campo y dejando atrás esa mala imagen que Facebook consiguió.</p>

<p><a href="https://imagebind.metademolab.com">https://imagebind.metademolab.com</a></p>

<hr />

<h1 id="mitigar-el-riesgo-de-extinción-humana-por-ia-debería-ser-una-prioridad-global">Mitigar el riesgo de extinción humana por IA debería ser una prioridad global</h1>
<p>Esto es lo que han dicho algunas figuras relevantes en el campo de la IA como los ganadores del premio Turing Geoffery Hinton y Yoshua Bengio así como el CEO de OpenAI Sam Altman, y otros ejecutivos de OpenAI como Ilya Sutskever y Mira Murati. Además también lo suscriben el CEO de DeepMind Demis Hassabis o el CEO de Anthropic Dario Amodei, y profesores de la UC Berkely, Stanford y MIT.</p>

<p>Esta vez no han escrito una <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">carta abierta</a> como sucedió en el pasado mes de marzo, sino que simplemente han dicho lo siguiente:</p>

<blockquote>
  <p><em>“Mitigar el riesgo de extinción por la IA debería ser una prioridad global junto con otros riesgos a escala social como las pandemias o las guerras nucleares.”</em></p>
</blockquote>

<p>Más información: <a href="https://www.safe.ai/press-release">https://www.safe.ai/press-release</a></p>

<p>¡Hasta la próxima semana!</p>]]></content><author><name></name></author><category term="2" /><summary type="html"><![CDATA[Según esta filtración, las gafas de Apple tendían unas especificaciones loquísimas Y otro rumor más… por si teníamos pocos, este último te dejará con la boca abierta (o más bien con los ojos abiertos) ya que se han filtrado las especificaciones de lo que serán las gafas de Apple y no tienen competencia… Serían estas:]]></summary></entry><entry><title type="html">Podcast 2x11</title><link href="https://vidageek.com/2/2023/05/27/podcast-2x11.html" rel="alternate" type="text/html" title="Podcast 2x11" /><published>2023-05-27T00:00:00+00:00</published><updated>2023-05-27T00:00:00+00:00</updated><id>https://vidageek.com/2/2023/05/27/podcast-2x11</id><content type="html" xml:base="https://vidageek.com/2/2023/05/27/podcast-2x11.html"><![CDATA[<h1 id="tree-of-thoughts-deliberate-problem-solving-with-large-language-models">Tree of thoughts: Deliberate problem solving with Large Language Models</h1>
<p><a href="https://arxiv.org/pdf/2305.10601.pdf">https://arxiv.org/pdf/2305.10601.pdf</a></p>

<p>Esta pasada semana se ha publicado un paper llamado “Tree of thoughts: Deliberate problem solving with Large Language Models” que sería algo como “Pensamiento arborescente: Solución de problemas con modelos grandes de lenguaje”. En este paper se explican los métodos más habituales en los que se usan los LLM como ChatGPT y se propone un nuevo método el cual mejora sustancialmente los métodos actuales.</p>

<p>Hasta ahora básicamente se usan 3 métodos:</p>
<ul>
  <li>Input-Output prompting: El cual consiste básicamente en hacer una pregunta para llegar a una respuesta. Sería lo que hacemos habitualmente con ChatGPT.</li>
  <li>Chain of thought prompting: Consiste en separar la pregunta en los pasos necesarios para llegar a una respuesta, de esta manera la respuesta siempre es más efectiva. Se utiliza en algunos proyectos como BabyAGI o AutoGPT.</li>
  <li>Self consistency with CoT: Que sería básicamente lo mismo que CoT pero haciendo la pregunta y elaborando los pasos varias veces de forma que nos quedaremos con la respuesta final que más se repita.</li>
</ul>

<p>Y el paper propone un nuevo método al que llama Tree of Thoughts o Árbol de pensamientos el cual consiste en, resumiéndolo mucho, en pedir al LLM que elabore varios planes para la resolución de un problema. Luego se le pide 5 veces que vote cuál es el mejor plan para resolver el problema planteado. Una vez hecho se le vuelve a pedir que elabore varios planes para resolver el problema basándose en los planes más votados y se vuelve a establecer un sistema de votaciones para esos planes generados. De esta manera se van creando unas ramas de pensamiento y las más votadas se van siguiendo. Es posible que alguna no llegue a buen puerto y se descarte esa rama por completo volviendo a una rama anterior.</p>

<p>Este nuevo método parece muy efectivo y es muy prometedor. En las pruebas que han realizado han conseguido unos resultados excelentes. Por ejemplo en la resolución de crucigramas, los métodos clásicos como IO o CoT solo conseguían resolver 1 de cada 100 en el mejor de los casos mientras que ToT ha conseguido resolver 20 de cada 100. E incluso es más impresionante en el juego de 24 en el cual los métodos tradicionales conseguían un 9% de éxito mientras que ToT consigue un 74% de soluciones correctas.</p>

<p>Ya ha salido una implementación en Python para que se pueda probar y trastear con ello:
<a href="https://github.com/kyegomez/tree-of-thoughts">https://github.com/kyegomez/tree-of-thoughts</a></p>

<h1 id="drag-your-gan-interactive-point-based-manipulation-on-the-generative-image-manifold">Drag your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</h1>
<p><a href="https://vcai.mpi-inf.mpg.de/projects/DragGAN/">https://vcai.mpi-inf.mpg.de/projects/DragGAN/</a></p>

<p>Este paper que se podría traducir como “Arrastra tu GAN, Manipulación interactiva basada en puntos de la variedad de imágenes generativas</p>

<hr />

<h1 id="mandos-de-videojuego-accesibles">Mandos de videojuego accesibles</h1>
<p>Hoy os traemos unos cuantos mandos económicos para poder disfrutar de nuestros juegos sin que nos escueza el bolsillo.</p>

<p>En concreto son estos modelos:
    - <a href="https://amzn.to/3C0N0mo">Microsoft Xbox original (XBOX y PC)</a>
    - <a href="https://amzn.to/3BVTR0l">Nacon - Compact Mando (PS4 y PC)</a>
    - <a href="https://amzn.to/3qiIvk9">G-Lab K-Pad Thorium Mando Gaming (PS3 y PC)</a>
    - <a href="https://amzn.to/3OGUySy">Diswoe Mando Xbox 360 (PC)</a>
    - <a href="https://amzn.to/3BXoYIT">PowerA (Xbox Series X|S Y PC)</a></p>

<h2 id="recomendacion">Recomendacion</h2>
<p>Desde mi punto de vista eligiria de todos ellos el mando POWER A por su calidad de acabado, precio y ergonomia a la hora de jugar ya que es practicamente lo mismo que el de xbox original pero con cable. Eso lo hace mas ligero pero con el incoveniente de tenerlo que tener conectado por cable.</p>

<p>¡Hasta la próxima semana!</p>]]></content><author><name></name></author><category term="2" /><summary type="html"><![CDATA[Tree of thoughts: Deliberate problem solving with Large Language Models https://arxiv.org/pdf/2305.10601.pdf]]></summary></entry><entry><title type="html">Podcast 2x10</title><link href="https://vidageek.com/2/2023/05/20/podcast-2x10.html" rel="alternate" type="text/html" title="Podcast 2x10" /><published>2023-05-20T00:00:00+00:00</published><updated>2023-05-20T00:00:00+00:00</updated><id>https://vidageek.com/2/2023/05/20/podcast-2x10</id><content type="html" xml:base="https://vidageek.com/2/2023/05/20/podcast-2x10.html"><![CDATA[<h1 id="habla-con-tus-documentos">Habla con tus documentos</h1>
<p>Esta semana os explicamos cómo hablar con vuestros documentos. Para ello hacemos uso de diferentes herramientas como <a href="https://www.chatpdf.com?via=vidageek">ChatPDF</a> o <a href="https://github.com/imartinez/privateGPT">PrivateGPT</a>.</p>

<p>El modo como funcionan estas herramientas se divide en 3 partes:</p>

<ul>
  <li>
    <p>Extracción de texto
El primer paso es extraer todo el texto del documento y categorizarlo con metadatos como número de página, autor, etc.</p>
  </li>
  <li>
    <p>Extraer embeddings del texto
Este paso utiliza un modelo que analiza semánticamente el texto y lo convierte a vectores de forma que puede ser indexado en una base de datos vectorial como <a href="https://pinecone.io">Pinecone</a> o similar.</p>
  </li>
  <li>
    <p>“Conversar” con el documento
Este último paso realiza el paso anterior por cada consulta que le realicemos. Es decir, genera embeddings de la consulta que queremos hacer para luego poder hacer una búsqueda en la base de datos de forma que encontrará los textos que se acerquen más de forma semántica a la consulta para utilizarlos como contexto o referencia en la consulta final.</p>
  </li>
</ul>

<h2 id="chatpdf">ChatPDF</h2>
<p><a href="https://www.chatpdf.com?via=vidageek">ChatPDF</a> es una herramienta online que utiliza por detrás los modelos de lenguaje de <a href="https://openai.com">OpenAI</a> como ChatGPT para las consultas o ADA para los embeddings. Es un servicio gratuito con limitaciones de hasta 120 páginas por documento o 3 documentos por día. Es muy sencilla de utilizar pero como inconveniente tiene que todo es procesdo online de forma que, si trabajamos con documentos sensibles o confidenciales, no deberíamos utilizarla.</p>

<h2 id="privategpt">PrivateGPT</h2>
<p>Por el contrario el proyecto <a href="https://github.com/imartinez/privateGPT">PrivateGPT</a> utiliza un modelo de lenguaje de código abierto que se ejecuta en local, directamente en tu CPU. Esto tiene la gran ventaja de que podemos trabajar con documentos confidenciales pero como contrapartida es mucho más lento, dependiendo en gran medida de la potencia de la CPU.</p>

<p>Como ejemplo os hemos puesto un <a href="https://colab.research.google.com/drive/1ikKWtOx73NirUjf72mNHJt_Q9uKI7m61#scrollTo=rV2Ydj-tqX-P">Google Colab</a> para que podáis probarlo aunque os adelantamos que es especialmente lento al usar sólo la CPU…</p>

<hr />
<h1 id="amazfit-gtr-4-y-gts-4">Amazfit GTR 4 y GTS 4</h1>

<h2 id="acerca-de-estos-relojes">Acerca de estos relojes</h2>
<p>Dos relojes muy similares por dentro pero muy diferentes por fuera.
El <a href="https://amzn.to/3WmkgO4">Amazfit GTR 4</a> cuenta con una pantalla HD amoled de 1.43 pulgadas con una resolucion de 466x466 Pixeles. Cuenta además con una batería que prometen durar unos 15 días. Con sensores para deporte con 150 modos de entrenamiento, control del sueño etc. Una opcion muy buena para el dia a dia y con un tamaño y forma muy deportivo</p>

<p>Por otro lado, el <a href="https://amzn.to/3pWDVbs">Amazfit GTS 4</a>, al contrario que su hermano, es rectangular(el gtr es redondo) y es la mayor diferencia que podemos encontrar en el dado que sus sensores son iguales. El peso tambien al ser mas pequeño es menor, la bateria pasa lo mismo pero no por ello malo ni mucho menos.</p>

<p>Cuenta con una pantalla de 1.75 pulgadas, comentamos en directo que la pantalla del gtr 4 era mas grande pero como dicen los datos lo dijimos mal.</p>

<p>En ambos casos hay que usar la aplicacion de Amazfit para poder usar las funcionalidades del mismo con nuestro telefono movil.
Dos opciones de relojes muy recomentadables y mucho mas asequible que otras marcas y desde luego para el dia a dia no dudaria en comprarmelo</p>

<h2 id="ventajas">Ventajas</h2>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Bateria
- Pantalla
- Precio
</code></pre></div></div>

<h2 id="desventajas">Desventajas</h2>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- No cuenta con NFC
- Fácil de rallar
- Aplicacion de terceros 
</code></pre></div></div>

<p>¡Hasta la próxima semana!</p>]]></content><author><name></name></author><category term="2" /><summary type="html"><![CDATA[Habla con tus documentos Esta semana os explicamos cómo hablar con vuestros documentos. Para ello hacemos uso de diferentes herramientas como ChatPDF o PrivateGPT.]]></summary></entry></feed>